{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_05b import *\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3641)\n",
    "\n",
    "In this lesson we want to create a CUDA CNN. We start as usual reading the data and normalizing them with the helper function `normalize_to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to quickly normalize with the mean and standard deviation from our training set: this function takes the `normalize` function we previously created, and applies it to both the training and the validation set in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def normalize_to(train, valid):\n",
    "    m, s = train.mean(), train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = normalize_to(x_train, x_valid)\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it behaved properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0614e-05), tensor(1.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `DataBunch` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh, bs = 50, 512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refactor layers, it's useful to have a `Lambda` layer that can take a basic function and convert it to a layer you can put in `nn.Sequential`.\n",
    "\n",
    "**NB**: if you use a Lambda layer with a lambda function, your model won't pickle so you won't be able to save it with PyTorch. So it's best to give a name to the function you're using inside your Lambda (like flatten below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one takes the flat vector of size `bs x 784` and puts it back as a batch of images of 28 by 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_resize(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a simple CNN. We would like ot insert `mnist_resize()` into `nn.Sequential`, but PyTorch doesn't support this by default. By creating the `Lambda` layer, we can put the data resizing in the `Sequential` list. The `forward` method of the `Lambda` layer simply applies the function passed to its `__init__` method.\n",
    "It's just a convenience to use `Sequential` rather than creating a subclass of `nn.Module`.\n",
    "\n",
    "Note that we are using `stride=2` with `padding=2`. The numbers in the comment are the resulting sizes of the activation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(data):\n",
    "    return nn.Sequential(\n",
    "        Lambda(mnist_resize),  # mnist_resize() as a layer.\n",
    "        nn.Conv2d(1, 8, 5, padding=2, stride=2), nn.ReLU(),  # 14\n",
    "        nn.Conv2d(8, 16, 3, padding=1, stride=2), nn.ReLU(),  # 7\n",
    "        nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.ReLU(),  # 4\n",
    "        nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),  # 2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten),  # flatten() as a layer.\n",
    "        nn.Linear(32, data.c)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** check what `AdaptiveAvgPool2d` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Lambda()\n",
      "  (1): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (8): ReLU()\n",
      "  (9): AdaptiveAvgPool2d(output_size=1)\n",
      "  (10): Lambda()\n",
      "  (11): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_cnn_model(data)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic callbacks from the previous notebook. The use of `partial` is explained in a previous notebook, and has the result of returning callback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [Recorder, partial(AvgStatsCallback, accuracy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner(cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.013141875, tensor(0.3024)]\n",
      "valid: [0.6929439453125, tensor(0.7622)]\n",
      "CPU times: user 8.15 s, sys: 567 ms, total: 8.71 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a long time to run, so it's time to use a GPU. We need to put two things on the GPU: 1) the model, and specifically the model parameters, and 2) the inputs to the model. How do we do it? With a Callback.\n",
    "The `CudaCallback` defined below makes sure that the model the inputs and the targets are all on the same device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somewhat more flexible way\n",
    "device = torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CudaCallback` is initialized with a device `device`, and when `begin_fit()` is called, it moves the model to the device. Moreover, at the beginning of each batch, it copies `xb` and `yb` to the device. This is possible because in the `Runner` class, the definition of `one_batch()` is the following:\n",
    "\n",
    "```python\n",
    "def one_batch(self, xb, yb):\n",
    "    self.xb = xb\n",
    "    self.yb = yb\n",
    "    ...\n",
    "```\n",
    "\n",
    "In the callback the `begin_batch()` method is defined as follows:\n",
    "\n",
    "```python\n",
    "def begin_batch(self):\n",
    "    self.run.xb = self.xb.to(device)\n",
    "    self.run.yb = self.yb.to(device)\n",
    "```\n",
    "\n",
    "This function changes `xb` and `yb` in the `Runner` instance. The fact that `begin_batch()` accesses the Runner `run` is confusing to me: why do you want to reference things that never appear in the Callback definition? Doesn't this violate the orthogonality principle?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudaCallback(Callback):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def begin_fit(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def begin_batch(self):\n",
    "        self.run.xb = self.xb.to(self.device)\n",
    "        self.run.yb = self.yb.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, one can do the following, which looks much easier and understandable to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f046779d8249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Somewhat less flexible, but quite convenient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "# Somewhat less flexible, but quite convenient\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can redefine `CudaCallback` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CudaCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.model.cuda()\n",
    "\n",
    "    def begin_batch(self):\n",
    "        self.run.xb = self.xb.cuda()\n",
    "        self.run.yb = self.yb.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can append this new Callback to the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cbfs)\n",
    "cbfs.append(CudaCallback)\n",
    "print(cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner(cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that's definitely faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make it easier to create different kinds of architectures. First we can notice that we tend to repeat certain groups of operations, and we can put thesein a single function. Since we use a kernel size of 3 and a stride of 2 a lot in these models, let's set them as the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, nf, ks=3, stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, `get_cnn_model()` is sequential model that can work only on MNIST, since the first `Lambda` layer calls `mnist_resize()`. If we remove it we need something else to do the resizing, and the answer is, again, a callback. The rather terrible name below means that this callback transforms the independent variable (`X`) for a batch. In other words, we create a new callback that is initialied with a transformation function `tfm` that is applied to each batch when `begin_batch()` is called. This allows us to pass whatever function we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BatchTransformXCallback(Callback):\n",
    "    _order = 2\n",
    "\n",
    "    def __init__(self, tfm):\n",
    "        self.tfm = tfm\n",
    "\n",
    "    def begin_batch(self):\n",
    "        self.run.xb = self.tfm(self.xb)\n",
    "\n",
    "\n",
    "def view_tfm(*size):\n",
    "    def _inner(x):\n",
    "        return x.view(*((-1,) + size))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`view_tfm` is a closure that returns a function of `x` and applies a view adding (if I'm not mistaken) the batch dimension upfront.\n",
    "\n",
    "Now, instead of using the `Lambda` layer with `mnist_transform()` we can create a callback function using `partial` and passing `mnist_view` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1, 28, 28)\n",
    "cbfs.append(partial(BatchTransformXCallback, mnist_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `AdaptiveAvgPool`, this model can now work on any size input. `nfs` is a list of number of filters per layer (couldn't you just write `n_filters`?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [8, 16, 32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this trick we can create a generic `get_cnn_model()` that can return an arbitrary set of layers.\n",
    "\n",
    "`get_cnn_layers()` returns a list containing a list of `conv2d` layers with number of filters specified by `nfs`, followed by an `AdaptiveAgvPool2d`, flatten etc.\n",
    "\n",
    "`get_cnn_model()` takes a learner, a list of numbers of filters, and returns a `nn.Sequential` model based on the output of `get_cnn_layers()`.\n",
    "\n",
    "Note the line `conv2d(nfs[i], nfs[i+1], 5 if i == 0 else 3)`. This sets the kernel size for the first layer to 5, and for the successive layers to 3. Why? If we have 8 3x3 convolutional kernels, at each given position we multiply the 9 values in the filter with the 9 overlapping values in the image and we sum them. We have therefore one value per filter, for a total of 8 numbers. In other words, we started with 9 numbers (the pixel values, I guess) and we end up with 8 numbers. This means that we are doing little more than just reordering these numbers. There is no point in making your first layer just shuffling those numbers.\n",
    "\n",
    "ImageNet is a bit different, as we have 3 channels. A 3x3x3 kernel has 27 values. Typically, we start with 32 filters in the first convolutional layer. J.H. says that going from 27 to 32 is wasting information. For this reason, many ImaegeNet models make the first layer's kernel 7x7 and not 3x3, so that now it goes from 7x7x3 = 147 to 32.\n",
    "\n",
    "Following the same loging, in the MNIST case, by choosing a 5x5 kernel size for the first layer, we go from 25 to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_layers(data, nfs):\n",
    "    nfs = [1] + nfs\n",
    "    return [\n",
    "        conv2d(nfs[i], nfs[i+1], 5 if i == 0 else 3)\n",
    "        for i in range(len(nfs)-1)\n",
    "    ] + [\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten), \n",
    "        nn.Linear(nfs[-1], data.c)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_cnn_model(data, nfs):\n",
    "    return nn.Sequential(*get_cnn_layers(data, nfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function returns a `Learner` and a `Runner` with the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, \n",
    "               loss_func=F.cross_entropy):\n",
    "    if opt_func is None:\n",
    "        opt_func = optim.SGD\n",
    "    opt = opt_func(model.parameters(), lr=lr)\n",
    "    learn = Learner(model, opt, loss_func, data)\n",
    "    return learn, Runner(cb_funcs=listify(cbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs)\n",
    "learn, run = get_runner(model, data, lr=0.4, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks\n",
    "\n",
    "We have seen how to build models in a more flexible way, and we have seen how important how initialization, kernel size, learning rate etc. are for the final performance. How can we train it more stably, more quickly etc? How do we find out if it saturating somewhere etc?\n",
    "\n",
    "What if we replace `nn.Sequential` with our own `Sequential` class? We did it before adding the following method.\n",
    "\n",
    "```python\n",
    "def __call__(self, x):\n",
    "    for i, l in enumerate(self.layers):  # J.H. is using `l` as a variable...\n",
    "        x = x(l)\n",
    "```\n",
    "\n",
    "However, we now add two more lines that grab the mean and the std. deviation of the outputs and save them into some lists. Now the model is keeping track of these parameters. It is performing some layer-wise *telemetry*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to do some telemetry, and want the mean and standard deviation of each activations in the model. First we can do it manually like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.act_means = [[] for _ in layers]\n",
    "        self.act_stds = [[] for _ in layers]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for i, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "            self.act_means[i].append(x.data.mean())\n",
    "            self.act_stds[i].append(x.data.std())\n",
    "        return x\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_cnn_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-90b56569acd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mSequentialModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mget_cnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_cnn_layers' is not defined"
     ]
    }
   ],
   "source": [
    "model =  SequentialModel(*get_cnn_layers(data, nfs))\n",
    "learn,run = get_runner(model, data, lr=0.9, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look at the means and stds of the activations at the beginning of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.act_means:\n",
    "    plt.plot(l)\n",
    "plt.legend(range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens is that the mean of each layer gets exponentially bigger until they suddenly collapse, and again, and again, until eventually it starts training. It is true that it start training eventually, but there are many parameters in our model. Are we sure that, after \"jumping off the cliff\" they all go back to reasonable values? Maybe the majority of them has zero gradient now. We don't know.\n",
    "\n",
    "The same thing happens for the standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.act_stds:\n",
    "    plt.plot(l)\n",
    "plt.legend(range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 means. They look OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.act_means:\n",
    "    plt.plot(l[:10])\n",
    "plt.legend(range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviations, however, do not look OK. The first one is, but the second layer has a lower std. dev, and the third one still lower, until the last layer has a std. dev. really close to zero. Looking at the means and the std. dev. we see that the final layers are moving to regions where there is no activation and no gradient. They were moving into regions where there is some gradient, but by the time they get there, the gradient is so fast, that they fall of a cliff and they have to start over again. This is what we want to fix, and we know that the right way to deal with this problem is with a good initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.act_stds:\n",
    "    plt.plot(l[:10])\n",
    "plt.legend(range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Telemetry is clearly useful, but rewriting a `Sequential` class that can only monitor one small set of parameters is not a flexible approach. Here we cannot use callbacks because we don't have a callback that says \"when you calculate this layer, call back to our code\". We need to use a feature into PyTorch: *hooks*.\n",
    "\n",
    "Hooks are PyTorch objects you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook). They are very similar to callbacks in spirit.\n",
    "\n",
    "Hooks don't require us to rewrite the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4693)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs)\n",
    "learn, run = get_runner(model, data, lr=0.5, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reproduce the behaviour we saw in the previous section, we create two global variables to store the means and the std. devs for every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_means = [[] for _ in model]\n",
    "act_stds = [[] for _ in model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list. This is the function we \"call back to\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(i, mod, inp, outp):\n",
    "    act_means[i].append(outp.data.mean())\n",
    "    act_stds[i].append(outp.data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much a callback that is called when the forward pass is executed. You can also call `register_backward_hook()` that will register a function that will be executed when the backward pass is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(model):\n",
    "    m.register_forward_hook(partial(append_stats, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in act_means:\n",
    "    plt.plot(o)\n",
    "plt.legend(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hooks are so convenient, fastai has a `Hook` class. With this class we don't need to have the global variables any more, and the state can be stored inside the class.\n",
    "\n",
    "It's very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won't be properly released when your model is deleted.\n",
    "\n",
    "The `__del__()` method is called automatically when Python clears some memory, so when the code is done with the `Hook` instance, it will automatically call `self.remove()`, which in turn will remove the hook via `self.hook.remove()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4836)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def children(m):\n",
    "    return list(m.children())\n",
    "\n",
    "\n",
    "class Hook():\n",
    "    def __init__(self, m, f):  # `f` is the function to be registered\n",
    "        self.hook = m.register_forward_hook(partial(f, self))\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below appensd mean and std.dev. to the `Hook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook, 'stats'):\n",
    "        hook.stats = ([], [])\n",
    "    means, stds = hook.stats\n",
    "    means.append(outp.data.mean())\n",
    "    stds.append(outp.data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: In fastai we use a `bool` param to choose whether to make it a forward or backward hook. In the above version we're only supporting forward hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs)\n",
    "learn,run = get_runner(model, data, lr=0.5, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `[:4]` is because we want to look only at the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = [Hook(l, append_stats) for l in children(model[:4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hooks:\n",
    "    plt.plot(h.stats[0])\n",
    "    h.remove()\n",
    "plt.legend(range(4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Hooks class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design our own class that can contain a list of objects. It will behave a bit like a numpy array in the sense that we can index into it via:\n",
    "- a single index\n",
    "- a slice (like 1:5)\n",
    "- a list of indices\n",
    "- a mask of indices (`[True,False,False,True,...]`)\n",
    "\n",
    "The `__iter__` method is there to be able to do things like `for x in ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4972)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "export\n",
    "class ListContainer():\n",
    "    def __init__(self, items):\n",
    "        self.items = listify(items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, (int, slice)):\n",
    "            return self.items[idx]\n",
    "        if isinstance(idx[0], bool):\n",
    "            assert len(idx) == len(self)  # bool mask\n",
    "            return [o for m, o in zip(idx, self.items) if m]\n",
    "        return [self.items[i] for i in idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.items)\n",
    "\n",
    "    def __setitem__(self, i, o):\n",
    "        self.items[i] = o\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        del(self.items[i])\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self) > 10:\n",
    "            res = res[:-1] + '...]'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListContainer(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListContainer(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ListContainer(range(10))\n",
    "t[[1,2]], t[[False]*8 + [True,False]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to write a `Hooks` class that contains several hooks. We will also use it in the next notebook as a container for our objects in the data block API.\n",
    "\n",
    "The `__del__()` method calls `self.remove()` that removes all the hooks in the list.\n",
    "The fact that we can loop on `self` is because above we have defined a `ListContainer` which behaves somehow like a list, but also has some behaviors like NumPy. We can slice it since it has a `__getitem__()` but it can also take boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class Hooks(ListContainer):\n",
    "    def __init__(self, ms, f):\n",
    "        super().__init__([Hook(m, f) for m in ms])\n",
    "\n",
    "    def __enter__(self, *args):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.remove()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.remove()\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "\n",
    "    def remove(self):\n",
    "        for h in self:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs).cuda()\n",
    "learn,run = get_runner(model, data, lr=0.9, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = Hooks(model, append_stats)\n",
    "hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a batch of data, resize it and send it to cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.train_dl))\n",
    "x = mnist_resize(x).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the output of the first layer only and compute mean and std. dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model[0](x)\n",
    "p.mean(), p.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we initialize with Kaiming's initialization, things are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model:\n",
    "    if isinstance(l, nn.Sequential):\n",
    "        init.kaiming_normal_(l[0].weight)\n",
    "        l[0].bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model[0](x)\n",
    "p.mean(),p.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having given an `__enter__` and `__exit__` method to our `Hooks` class, we can use it as a context manager. This makes sure that onces we are out of the `with` block, all the hooks have been removed and aren't there to pollute our memory.\n",
    "\n",
    "Here we first plot the first 10 means and std. devs, and then we plot them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(model, append_stats) as hooks:\n",
    "    run.fit(2, learn)\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for h in hooks:\n",
    "        ms, ss = h.stats\n",
    "        ax0.plot(ms[:10])\n",
    "        ax1.plot(ss[:10])\n",
    "    plt.legend(range(6))\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for h in hooks:\n",
    "        ms, ss = h.stats\n",
    "        ax0.plot(ms)\n",
    "        ax1.plot(ss)\n",
    "    plt.legend(range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial quesition was about how many of the activations are really small, and how harmful is the \"cliffy\" behavior we saw initially.\n",
    "\n",
    "Let's store more than the means and stds and plot histograms of our activations now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook, 'stats'):\n",
    "        hook.stats = ([], [], [])\n",
    "    means, stds, hists = hook.stats\n",
    "    means.append(outp.data.mean().cpu())\n",
    "    stds .append(outp.data.std().cpu())\n",
    "    # histc isn't implemented on the GPU\n",
    "    hists.append(outp.data.cpu().histc(40, 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are setting a really large learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs).cuda()\n",
    "learn, run = get_runner(model, data, lr=0.9, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model:\n",
    "    if isinstance(l, nn.Sequential):\n",
    "        init.kaiming_normal_(l[0].weight)\n",
    "        l[0].bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(model, append_stats) as hooks:\n",
    "    run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to @ste for initial version of histgram plotting code\n",
    "def get_hist(h):\n",
    "    return torch.stack(h.stats[2]).t().float().log1p()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that even with the large learning rate we set, we have this spikey trend. The real concern, however, is the yellow line at the bottom. That's where most of the histogram concentrates. Here the `x` axis is the iteration, while the `y` axis is the activation level. The counts are shown by the color density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "for ax, h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.imshow(get_hist(h), origin='lower')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms, we can easily get more informations like the min or max of the activations. We take the sum of the content of the first two bins, and we divide it by the sum of all of the bins. This will tell us what percentage of the activations is zero or nearly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(h):\n",
    "    h1 = torch.stack(h.stats[2]).t().float()\n",
    "    return h1[:2].sum(0)/h1.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last layer, almost 90% of the activations are near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "for ax, h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0, 1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous setting, we are throwing away 90% of the activations. Let's try to fix this. We are going to try a few things, but first fo all we will use our model with a generalized ReLU that can be shifted and with maximum value.\n",
    "\n",
    "We can subtract an amount `sub` from the ReLU, as we saw that subtracting 0.5 seemed to be beneficial. We can have leakage, and we can also add a maximum value to clamp. We pass this `GeneralRelu` to the function that generates the `Sequential` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_cnn_layers(data, nfs, layer, **kwargs):\n",
    "    nfs = [1] + nfs\n",
    "    return [layer(nfs[i], nfs[i+1], 5 if i == 0 else 3, **kwargs)\n",
    "            for i in range(len(nfs)-1)] + [\n",
    "        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]\n",
    "\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=2, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs))\n",
    "\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak = leak\n",
    "        self.sub = sub\n",
    "        self.maxv = maxv \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(x, self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None:\n",
    "            x.sub_(self.sub)\n",
    "        if self.maxv is not None:\n",
    "            x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "\n",
    "def init_cnn(m, uniform=False):\n",
    "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
    "    for l in m:\n",
    "        if isinstance(l, nn.Sequential):\n",
    "            f(l[0].weight, a=0.1)\n",
    "            l[0].bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_cnn_model(data, nfs, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the limits of the histograms, since the new ReLU now allows negative values. We also need to change the definition of `get_min` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook, 'stats'):\n",
    "        hook.stats = ([], [], [])\n",
    "    means, stds, hists = hook.stats\n",
    "    means.append(outp.data.mean().cpu())\n",
    "    stds.append(outp.data.std().cpu())\n",
    "    hists.append(outp.data.cpu().histc(40, -7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we subtract 0.4, not 0.5, because the leaky ReLU compensates for it, somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs, conv_layer, leak=0.1, sub=0.4, maxv=6.)\n",
    "init_cnn(model)\n",
    "learn, run = get_runner(model, data, lr=0.9, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(model, append_stats) as hooks:\n",
    "    run.fit(1, learn)\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for h in hooks:\n",
    "        ms, ss, hi = h.stats\n",
    "        ax0.plot(ms[:10])\n",
    "        ax1.plot(ss[:10])\n",
    "        h.remove()\n",
    "    plt.legend(range(5))\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for h in hooks:\n",
    "        ms, ss, hi = h.stats\n",
    "        ax0.plot(ms)\n",
    "        ax1.plot(ss)\n",
    "    plt.legend(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "for ax, h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.imshow(get_hist(h), origin='lower')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(h):\n",
    "    h1 = torch.stack(h.stats[2]).t().float()\n",
    "    return h1[19:22].sum(0)/h1.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. How many dead activations do we now have in the final layers? Less than 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "for ax, h in zip(axes.flatten(), hooks[:4]):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0, 1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):\n",
    "    model = get_cnn_model(data, nfs, layer, **kwargs)\n",
    "    init_cnn(model, uniform=uniform)\n",
    "    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks good. What do we get if we train with a 1-cycle learning where we use what we previously build?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = combine_scheds([0.5, 0.5], [sched_cos(0.2, 1.), sched_cos(1., 0.1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs+[partial(ParamScheduler,'lr', sched)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(8, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get to ~98%!\n",
    "\n",
    "If you look at the definition of `init_cnn` there is a `uniform=False` argument. If set to `True`, it uses the Kaiming *uniform* initialization, instead of the Kaiming *normal* one.\n",
    "\n",
    "Uniform init may provide more useful initial weights (normal distribution puts a lot of them at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 1., conv_layer, uniform=True,\n",
    "                          cbs=cbfs+[partial(ParamScheduler,'lr', sched)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(8, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy way to export our module without needing to update the file name - after we define this, we can just use `nb_auto_export()` in the future (h/t Stas Bekman):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from IPython.display import display, Javascript\n",
    "def nb_auto_export():\n",
    "    display(Javascript(\"\"\"{\n",
    "const ip = IPython.notebook\n",
    "if (ip) {\n",
    "    ip.save_notebook()\n",
    "    console.log('a')\n",
    "    const s = `!python notebook2script.py ${ip.notebook_name}`\n",
    "    if (ip.kernel) { ip.kernel.execute(s) }\n",
    "}\n",
    "}\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_auto_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
