{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_03 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBunch/Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4799)\n",
    "\n",
    "Most library have callbacks, but fastai callbacks are more pervasive, and they are used in many ways withing the library. The fastai v1 training loop contains a bunch of calls to callbacks, like `on_train_begin`, `on_epoch_begin`, `on_batch_begin` etc. With these, fasta v1 creates callbacks for learning rate schedulers, early stopping, parallel trainers, gradient clipping. You can mix them all together and write something like:\n",
    "\n",
    "> `learn.fit(epochs, lr, wd, callbacks)`\n",
    "\n",
    "For example, GANs are modules (`GANModule` class) that have an `nn.Module` object called `generator` passed to their `__init__`. In the forward pass, we inspect the (boolean) value of `generator`. If `True`, we are in generator mode, otherwise we are in discriminator mode (called *critic* in the library). There is then a `switch` method that switches between generator and discriminator mode. Similarly, they create a `GANLoss` class containing a generator loss and a discriminator/critic loss. They then create a `GANTrainer` class that inherits from `LearnerCallback` containing `on_train_begin`, `on_train_end`, etc., where each method does the required thing. (You need to go through the code to understand how it works. J.H. is just showing screenshots of these classes).\n",
    "\n",
    "From the previous notebook we have a training loop. Let's start again grabbing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "nh, bs = 50, 64  # N. of hidden units and batch size.\n",
    "c = y_train.max().item() + 1  # N. of classes.\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the current signature of our `fit` function. It contains a lot of parameters, which is usually a *code smell*. Can we package some of these things together? This is usually a good thing, as we can pass them around together, create factory methods to create them etc. \n",
    "\n",
    "`fit(epochs, model, loss_func, opt, train_dl, valid_dl)`\n",
    "\n",
    "We can do this in two steps: first we observe that training and validation set should live in one place, as they are both processed in the training loop. We put these datasets into a `DataBunch`. Then we create another class that stores the `DataBunch`, the loss function, the optimizer and the model, and we call it a `Learner`. Our final goal is to have something like this:\n",
    "\n",
    "`fit(1, learn)`\n",
    "\n",
    "This will allow us to tweak what's happening inside the training loop in other places of the code because the `Learner` object will be mutable, so changing any of its attribute elsewhere will be seen in our training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5363)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataBunch` stores the training and the validation `DataLoader`s, and has a couple of properties that return the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c=None):\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.c = c\n",
    "\n",
    "    @property\n",
    "    def train_ds(self):\n",
    "        return self.train_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def valid_ds(self):\n",
    "        return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_dls` function simply taks two datsets, a batch size, and possibly some additional keyword arguments, and returns a tuple of `DataLoader`s. This is the body of the function.\n",
    "\n",
    "```python\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `get_model` function that returns a simple sequential model and an SGD optimizer. This is just for illustrative purposes. One nice aspect of this function is that it sets the dimension of the last size to the correct value, by useing the `data.c` attribute.\n",
    "\n",
    "The important bit is the `Learner` class, that bundles together the model, the optimizer, the loss function and the `DataBunch`. The `Learner` class has no logic at all. It is only a storage device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model(data, lr=0.5, nh=50):\n",
    "    m = data.train_ds.x.shape[1]\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.loss_func = loss_func\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as the previous `fit` function, but wherever we had `model` we now have `learn.model`; where we had `data` we now have `learn.data`; where we had `opt` we have `learn.opt`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        for xb, yb in learn.data.train_dl:\n",
    "            loss = learn.loss_func(learn.model(xb), yb)\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb)\n",
    "                tot_acc += accuracy(pred, yb)\n",
    "        nv = len(learn.data.valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3741) tensor(0.8812)\n"
     ]
    }
   ],
   "source": [
    "loss, acc = fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was our training loop (without validation) from the previous notebook, with the inner loop contents factored out. More precisely we have factored out the computations in the `one_batch` function.\n",
    "\n",
    "```python\n",
    "def one_batch(xb,yb):\n",
    "    pred = model(xb)\n",
    "    loss = loss_func(pred, yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for b in train_dl:\n",
    "            one_batch(*b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually add a `all_batches` function, that runs `one_batch` on all batches both on the training and in the validation set. We add callbacks so we can remove complexity from loop, and make it flexible:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch(xb, yb, cb):\n",
    "    if not cb.begin_batch(xb, yb):\n",
    "        return\n",
    "    loss = cb.learn.loss_func(cb.learn.model(xb), yb)\n",
    "    if not cb.after_loss(loss):\n",
    "        return\n",
    "    loss.backward()\n",
    "    if cb.after_backward():\n",
    "        cb.learn.opt.step()\n",
    "    if cb.after_step():\n",
    "        cb.learn.opt.zero_grad()\n",
    "\n",
    "\n",
    "def all_batches(dl, cb):\n",
    "    for xb, yb in dl:\n",
    "        one_batch(xb, yb, cb)\n",
    "        if cb.do_stop():\n",
    "            return\n",
    "\n",
    "\n",
    "def fit(epochs, learn, cb):\n",
    "    if not cb.begin_fit(learn):\n",
    "        return\n",
    "    for epoch in range(epochs):\n",
    "        if not cb.begin_epoch(epoch):\n",
    "            continue\n",
    "        all_batches(learn.data.train_dl, cb)\n",
    "\n",
    "        if cb.begin_validate():\n",
    "            with torch.no_grad():\n",
    "                all_batches(learn.data.valid_dl, cb)\n",
    "        if cb.do_stop() or not cb.after_epoch():\n",
    "            break\n",
    "    cb.after_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom callback inherits from the class below, which defines the methods we may use, and returns `True` by default. Some methods, like `begin_fit`, `begin_epoch`, `begin_batch`, `after_loss`, also store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn = learn\n",
    "        return True\n",
    "\n",
    "    def after_fit(self):\n",
    "        return True\n",
    "\n",
    "    def begin_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        return True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        return True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        return True\n",
    "\n",
    "    def begin_batch(self, xb, yb):\n",
    "        self.xb = xb\n",
    "        self.yb = yb\n",
    "        return True\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        self.loss = loss\n",
    "        return True\n",
    "\n",
    "    def after_backward(self):\n",
    "        return True\n",
    "\n",
    "    def after_step(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `CallbackHandler` is initialized with a list of Callbacks, if any, and then it takes care of running them all in the order they appear in the list. For example, `begin_fit` takes a learner, sets `in_train` and `res` to `True` and `stop` to `False`, and loops over all the callbacks. For each of them, it calls the `begin_git(learn)` method. At the end of the loop it returns the final valud of `res`.\n",
    "\n",
    "The important thing is to understand how the callbacks, callback handler and training loop come together.\n",
    "\n",
    "1. We create a learner.\n",
    "2. We create one or more `Callback`.\n",
    "3. We pass the list of callbacks to a `CallbackHandler`.\n",
    "4. We pass the learner and the callback handler to the `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackHandler():\n",
    "    def __init__(self, cbs=None):\n",
    "        self.cbs = cbs if cbs else []\n",
    "\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn = learn\n",
    "        self.in_train = True\n",
    "        learn.stop = False\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.begin_fit(learn)\n",
    "        return res\n",
    "\n",
    "    def after_fit(self):\n",
    "        res = not self.in_train\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.after_fit()\n",
    "        return res\n",
    "\n",
    "    def begin_epoch(self, epoch):\n",
    "        self.learn.model.train()\n",
    "        self.in_train = True\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.begin_epoch(epoch)\n",
    "        return res\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.learn.model.eval()\n",
    "        self.in_train = False\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.begin_validate()\n",
    "        return res\n",
    "\n",
    "    def after_epoch(self):\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.after_epoch()\n",
    "        return res\n",
    "\n",
    "    def begin_batch(self, xb, yb):\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.begin_batch(xb, yb)\n",
    "        return res\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        res = self.in_train\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.after_loss(loss)\n",
    "        return res\n",
    "\n",
    "    def after_backward(self):\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.after_backward()\n",
    "        return res\n",
    "\n",
    "    def after_step(self):\n",
    "        res = True\n",
    "        for cb in self.cbs:\n",
    "            res = res and cb.after_step()\n",
    "        return res\n",
    "\n",
    "    def do_stop(self):\n",
    "        try:\n",
    "            return self.learn.stop\n",
    "        finally:\n",
    "            self.learn.stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the `TestCallback` callback below inherits `begin_fit` from the `Callback` class, and sets the `n_iters` attribute to zero. At the end of each step, it increases `n_iters` by one. If the number of iteration equals 10, it stops the training. This can be a handy callback, since we may often want to run just a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def begin_fit(self, learn):\n",
    "        super().begin_fit(learn)\n",
    "        self.n_iters = 0\n",
    "        return True\n",
    "\n",
    "    def after_step(self):\n",
    "        self.n_iters += 1\n",
    "        print(self.n_iters)\n",
    "        if self.n_iters >= 10:\n",
    "            self.learn.stop = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "fit(1, learn, cb=CallbackHandler([TestCallback()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is roughly how fastai does it now (except that the handler can also change and return `xb`, `yb`, and `loss`). But let's see if we can make things simpler and more flexible, so that a single class has access to everything and can change anything at any time. The fact that we're passing `cb` to so many functions is a strong hint they should all be in the same class! The fact that we are passing `cb` everywhere indicates that we should keep this state somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner\n",
    "\n",
    "A `Runner` is a new class that is likely to appear in the new version of fastai, and that contains the three things we have been using so far: `one_batch`, `all_batches` and `fit`.\n",
    "\n",
    "**Python Note**: whenever a Python function or method does *not* return a value, this is equivalent to returning `None` which evaluates to `False` in logical operations. In all the code below, the logic is:\n",
    "\n",
    "> If your callback handler returns `False`, *keep going*.\n",
    "\n",
    "This means that the callbacks don't need to return anything, most of the time. `TestCallback`, for example, returns `True` when we want to *stop* the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def foo(x):\n",
    "    x +=1\n",
    "\n",
    "\n",
    "print(foo(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5811)\n",
    "\n",
    "We re-write the `Callback` class adding an `_order` class attribute. We also add a `set_runner` method that assigns the runner as an attribute and redefine the `__getattr__` method. Note that `__getattr__` is called only if the attribute `k` is *not* found.\n",
    "\n",
    "When in the `Runner` class below we call the `fit` method, this goes through the callbacks in the callback list and calls their `set_runner` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "\n",
    "\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a new `Callback` class which is much shorter and simpler than the previous one. We will come back to the `_order` attribute later, but note for now the presence of `__getattr__()`. It is defined to return `getattr(self.run, k)`. `__getattr__` is only called by Python when it *cannot* find the attribute it is asked for. So if we ask for an attribute `k` and this does not exist, `__getattr__(self, k)` will call `getattr(self.run, k)`, *i.e.*, if the attribute `k` is not defined in the `Callback`, it will look for it into the `Runner`. This is a common pattern in fastai. If an object contains or composes another object, we very often delegate `gettatr` to the other object.\n",
    "\n",
    "The `Callback` class has a `name` property. This works as follows: if we create a new callback called `MyOwnCallback`, this will have a name that is the name of the class converted to snake case and stripped of the `callback` part, *i.e.*, `my_own`. This is useful because in `Runner` we have the following code:\n",
    "\n",
    "```python\n",
    "def __init__(self, cbs=None, cb_funcs=None):\n",
    "    cbs = listify(cbs)\n",
    "    for cbs in listify(cbf_funcs):\n",
    "        cb = cbf()\n",
    "        setattr(self, cb.name, cb)  # <--- here\n",
    "        cb.append(cb)\n",
    "    self.stop = False\n",
    "    self.cbs = [TrainEvalCallback()] + cbs\n",
    "```\n",
    "\n",
    "That line sets the attribute in the runner from the callback. This is used, for example, for the `Recorder`.\n",
    "\n",
    "We use this to add metrics, via the `AvgStatsCallback` class. At the beginning of an epoch (`.begin_epoch()`) we reset the statistics and at the end of an epoch (`.after_epoch()`) we print them. After computing the loss (`.after_loss()`) we accumulate them. In order to do this our `AvgStatsCallback` class has an `accumulate()` method. There are also two properties `all_stats` and `avg_stats` that return what their names say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    _order = 0\n",
    "\n",
    "    def set_runner(self, run):\n",
    "        self.run = run\n",
    "\n",
    "    def __getattr__(self, k):\n",
    "        return getattr(self.run, k)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first callback is reponsible to switch the model back and forth in training or validation mode, as well as maintaining a count of the iterations, or the percentage of iterations ellapsed in the epoch. More details about this class are given below. The way the methods are organized, in fact, can only be understood in the light of the `Runner` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs = 0.\n",
    "        self.run.n_iter = 0\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train:\n",
    "            return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter += 1\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs = self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train = True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same `TestCallback` as before, but notice how much smaller it has become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def after_step(self):\n",
    "        if self.train_eval.n_iters >= 10:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval_callback'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbname = 'TrainEvalCallback'\n",
    "camel2snake(cbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainEvalCallback().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import *\n",
    "\n",
    "\n",
    "def listify(o):\n",
    "    if o is None:\n",
    "        return []\n",
    "    if isinstance(o, list):\n",
    "        return o\n",
    "    if isinstance(o, str):\n",
    "        return [o]\n",
    "    if isinstance(o, Iterable):\n",
    "        return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method in the runner is very simple: it takes an `epoch` and a `learner` (which has no logic in it), we then tell each of our callbacks what runner they are currently working with, and we then call `begin_fit`, we go through each epoch in turn starting with a call to `begin_epoch` which , unless it returns `False`, calls `all_batches(self.data.train_dl)`. Once the training set is exhausted we pass to the validatin step within a `no_grad()` context manager, call `begin_validate()` and run `all_batches(self.data.valid_dl)`.\n",
    "\n",
    "Calls like `self('begin_fit')`, `self('begin_epoch')`, `self('after_fit')` etc. may look strange. What J.H. has done has been to define a `__call__` method that, as usual, lets you treat an object as if it were a function. In one of the next lessons we will see that we could have called this `callback` or `run_callbacks` or something else. Here, J.H. prefers to use `self` as a function, but I personally find that it makes the code much less readable. `__call__` will go through all the callbacks. J.H. says that he didn't like the fact that before each callback had to inherit from the `Callback` class, in order to have the various methods (what's wrong with that?), so this is why he is now using `__getattr__(cb, cb_name, None)` which means: \"look inside `cb` and check whether the name `cb_name` exists, and if you cannot find it return `None`. If it finds it, it calls it (very unfortunately) `f` and runs it.\n",
    "\n",
    "This trick allows us to re-write our `TestCallback` in a much simpler way. This happens because we can refer to stuff that is defined inside the runner and call `self.iter`.\n",
    "\n",
    "Technically we don't need to inherit from `Callback` any more, but we still do it for one detail: `_order`. This allows us to decide in which order we run our callbacks. Often you need things to run in a particular order. In the `__call__` method of the `Runner` there is the following line\n",
    "\n",
    "```python\n",
    "for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "```\n",
    "\n",
    "which sorts the callbacks by the `_order` attribute. For example, we may want to execute `TrainEvalCallback` (see below) first, assignign it `_order = 0` and our `TestCallback` afterward, with `_order = 1`.\n",
    "\n",
    "You may have noticed that the `Runner` never calls `model.train` or `model.eval`. This is because there is a `TrainEvalCallback` that, at the beginning of an epoch, when `begin_epoch()` is called, calls `model.train()`, and at the beginning of the validation phase, when `begin_validate()` is called, calls `model.eval()`. `TrainEvalCallback` has also something that keep tracks of the epoch express as a float. This happens inside `after_batch()`. It also keeps track of the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop = False\n",
    "        self.cbs = [TrainEvalCallback()] + cbs\n",
    "\n",
    "    @property\n",
    "    def opt(self):\n",
    "        return self.learn.opt\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.learn.model\n",
    "\n",
    "    @property\n",
    "    def loss_func(self):\n",
    "        return self.learn.loss_func\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb, self.yb = xb, yb\n",
    "        if self('begin_batch'):\n",
    "            return\n",
    "        self.pred = self.model(self.xb)\n",
    "        if self('after_pred'):\n",
    "            return\n",
    "        self.loss = self.loss_func(self.pred, self.yb)\n",
    "        if self('after_loss') or not self.in_train:\n",
    "            return\n",
    "        self.loss.backward()\n",
    "        if self('after_backward'):\n",
    "            return\n",
    "        self.opt.step()\n",
    "        if self('after_step'):\n",
    "            return\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        for xb, yb in dl:\n",
    "            if self.stop:\n",
    "                break\n",
    "            self.one_batch(xb, yb)\n",
    "            self('after_batch')\n",
    "        self.stop = False\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs, self.learn = epochs, learn\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs:\n",
    "                cb.set_runner(self)\n",
    "            if self('begin_fit'):\n",
    "                return\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'):\n",
    "                    self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'):\n",
    "                        self.all_batches(self.data.valid_dl)\n",
    "                if self('after_epoch'):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, cb_name, None)\n",
    "            if f and f():\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third callback: how to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train):\n",
    "        self.metrics = listify(metrics)\n",
    "        self.in_train = in_train\n",
    "\n",
    "    def reset(self):\n",
    "        self.tot_loss = 0.\n",
    "        self.count = 0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "\n",
    "    @property\n",
    "    def all_stats(self):\n",
    "        return [self.tot_loss.item()] + self.tot_mets\n",
    "\n",
    "    @property\n",
    "    def avg_stats(self):\n",
    "        return [o/self.count for o in self.all_stats]\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self.count:\n",
    "            return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        for i, m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
    "\n",
    "\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats = AvgStats(metrics, True)\n",
    "        self.valid_stats = AvgStats(metrics, False)\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "\n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad():\n",
    "            stats.accumulate(self.run)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we create our `AvgStatsCallback` and then pass it to `run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = AvgStatsCallback([accuracy])\n",
    "run = Runner(cbs=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.3159608203125, tensor(0.9019)]\n",
      "valid: [0.282188720703125, tensor(0.9173)]\n",
      "train: [0.140204130859375, tensor(0.9574)]\n",
      "valid: [0.12210206298828125, tensor(0.9634)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12210206298828125, tensor(0.9634))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = stats.valid_stats.avg_stats\n",
    "assert acc > 0.9\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing this is to create an accuracy **callback function**. We do this by using `partial` from `functools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the operation below is a **function that can create a callback**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_cbf = partial(AvgStatsCallback, accuracy)\n",
    "type(acc_cbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass `acc_cbf` to `Runner`, we don't need to store the `stats` callback into the runner anymore, because the runner will run this bit of code\n",
    "\n",
    "```python\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):  # Go through each callback function\n",
    "            cb = cbf()                 # Call the function and create the callback.\n",
    "            setattr(self, cb.name, cb) # Save the callback as an attribute in the runner with name `cb.name`.\n",
    "            cbs.append(cb)\n",
    "        self.stop = False\n",
    "        self.cbs = [TrainEvalCallback()] + cbs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Runner(cb_funcs=acc_cbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.107763583984375, tensor(0.9668)]\n",
      "valid: [0.2064674072265625, tensor(0.9432)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `partial` we can now access the statistics from inside the runner. This is what fastai v1 does, with the difference that the statistics are saved in a `Learner` and not in a `Runner` (there is no `Runner` class in v1).\n",
    "\n",
    "Using Jupyter means we can get tab-completion even for dynamic code like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2064674072265625, tensor(0.9432)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.avg_stats.valid_stats.avg_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approaches seen so far may look awkward at the beginning, but it's up to the user how deep s/he wants to go into the Software Engineering details of these solutions. The important bit to retain is summarized by this chunck of code:\n",
    "\n",
    "```python\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs, self.learn = epochs, learn\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs:\n",
    "                cb.set_runner(self)\n",
    "            if self('begin_fit'):\n",
    "                return\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'):\n",
    "                    self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'):\n",
    "                        self.all_batches(self.data.valid_dl)\n",
    "                if self('after_epoch'):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 04_callbacks.ipynb to exp/nb_04.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 04_callbacks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
